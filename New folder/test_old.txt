import numpy as np
from RRTTree import RRTTree
import time


class RRTPlanner(object):

    def __init__(self, planning_env, ext_mode, goal_prob):

        # set environment and search tree
        self.planning_env = planning_env
        self.tree = RRTTree(self.planning_env)

        # set search params
        self.ext_mode = ext_mode
        self.goal_prob = goal_prob
        self.nums_of_runs = 10          # added in order to provide statistical results (averages over 10 runs).

    def plan(self):
        '''
        Compute and return the plan. The function should return a numpy array containing the states (positions) of the robot.
        '''
        start_time = time.time()

        # initialize an empty plan.
        plan = []

        # TODO: Task 3
        env = self.planning_env
        start_state = env.start             # initialize start state
        goal_state = env.goal               # initialize goal state
        self.tree.add_vertex(start_state)   # create first node in the tree
        avg_cost = 0
        avg_time = 0
        runs_num = self.nums_of_runs
        for cnt in range(runs_num):
            total_cost = 0
            total_time = 0
            while not self.tree.is_goal_exists(state = goal_state):
                p = np.random.uniform()

                # Sample a random state with goal bias
                if p < self.goal_prob:
                    sampled_state = env.goal
                else:
                    x_coord = np.random.uniform(env.xlimit[0], env.xlimit[1])
                    y_coord = np.random.uniform(env.ylimit[0], env.ylimit[1])
                    sampled_state = np.array([x_coord, y_coord])

                # Find the nearest state in the tree
                [nearest_neighbor_id, nearest_neighbor_state] = self.tree.get_nearest_state(state=sampled_state)
                added_state = self.extend(nearest_neighbor_state, sampled_state)

                # check if the edge between two states is free from collisions
                is_valid_cond_1 = env.state_validity_checker(state = added_state)
                is_valid_cond_2 = env.edge_validity_checker(state1 = nearest_neighbor_state, state2 = added_state)

                # If valid, add vertex and compute cost and create edge
                if is_valid_cond_1 and is_valid_cond_2:
                    self.tree.add_vertex(state = added_state)
                    cost = env.compute_distance(start_state = nearest_neighbor_state, end_state = added_state)
                    nearest_new_id = self.tree.get_idx_for_state(state=added_state)
                    self.tree.add_edge(sid=nearest_neighbor_id, eid = nearest_new_id, edge_cost = cost)

                # Search for the goal vertex and return the index if exists
                curr_state_id = self.tree.get_idx_for_state(state = goal_state)

                # Loop from goal vertex till we get to the root, and append to plan each vertex.
            while curr_state_id != self.tree.get_root_id():
                curr_state = self.tree.vertices[curr_state_id].state
                plan.append(curr_state)
                curr_state_id = self.tree.edges[curr_state_id]

            #insert the root
            curr_state = self.tree.vertices[curr_state_id].state
            plan.append(curr_state)

            #reverse the plan so the root will be at the start.
            plan.reverse()



            # print total path cost and time
            total_cost = self.compute_cost(plan)
            run_time = time.time() - start_time
            print('Total cost of path (run {}): {:.2f}'.format(cnt + 1, total_cost))
            print('Total time (run {}): {:.2f}'.format(cnt + 1, run_time))
            avg_cost += total_cost
            avg_time += run_time

        print('Calc plan for:')
        print(f'Goal prob: {self.goal_prob}')
        print(f'Extend mode: {self.ext_mode}')

        if self.ext_mode == 'E2':
            print(f'tested eta: {self.eta}')

        avg_cost = avg_cost / runs_num
        avg_time = avg_time / runs_num
        print('Avg cost of path: {:.2f}'.format(avg_cost))
        print('Avg time: {:.2f}'.format(avg_time))


        return np.array(plan)


    def compute_cost(self, plan):
        '''
        Compute and return the plan cost, which is the sum of the distances between steps.
        @param plan A given plan for the robot.
        '''
        # TODO: Task 3
        cost = 0
        cnt = 0
        env = self.planning_env
        while cnt < (len(plan) - 1):
            cost += env.compute_distance(start_state=plan[cnt], end_state=plan[cnt + 1])
            cnt += 1
        return cost


    def extend(self, near_state, rand_state):
        '''
        Compute and return a new position for the sampled one.
        @param near_state The nearest position to the sampled position.
        @param rand_state The sampled position.
        '''
        # TODO: Task 3
        # Check which mode is requested
        env = self.planning_env
        if self.ext_mode == 'E1':
            return rand_state
        elif self.ext_mode == 'E2':
            # we want to choose the best eta
            eta_vec = [5, 10, 15]
            # the best result was received when eta = 10
            tested_eta = eta_vec[1]
            self.eta = tested_eta

            # Compute the vector difference (delta) between the sampled point and the nearest state.
            delta = (rand_state - near_state)

            # Calculate the Euclidean distance between the sampled state and the nearest state.
            # This is used to normalize the delta vector and ensure the extension is properly scaled.
            delta_norm = env.compute_distance(start_state=rand_state, end_state=near_state)

            # Normalize the direction vector (delta) so that it points in the same direction as the difference,but with a unit length.
            normalized_direction_vector = delta / delta_norm

            new_state = rand_state

            # If the distance is greater than eta, we move towards the sampled state
            if self.eta < delta_norm:
                new_state = near_state + self.eta * normalized_direction_vector
            return new_state
        else:
            # In case of wrong extended mode
            assert 0